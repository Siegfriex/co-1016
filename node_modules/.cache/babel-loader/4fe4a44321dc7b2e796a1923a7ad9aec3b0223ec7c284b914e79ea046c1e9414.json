{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../../resource.mjs\";\nexport class Methods extends APIResource {}","map":{"version":3,"names":["APIResource","Methods"],"sources":["C:\\co-1016\\node_modules\\openai\\src\\resources\\fine-tuning\\methods.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\r\n\r\nimport { APIResource } from '../../resource';\r\nimport * as GraderModelsAPI from '../graders/grader-models';\r\n\r\nexport class Methods extends APIResource {}\r\n\r\n/**\r\n * The hyperparameters used for the DPO fine-tuning job.\r\n */\r\nexport interface DpoHyperparameters {\r\n  /**\r\n   * Number of examples in each batch. A larger batch size means that model\r\n   * parameters are updated less frequently, but with lower variance.\r\n   */\r\n  batch_size?: 'auto' | number;\r\n\r\n  /**\r\n   * The beta value for the DPO method. A higher beta value will increase the weight\r\n   * of the penalty between the policy and reference model.\r\n   */\r\n  beta?: 'auto' | number;\r\n\r\n  /**\r\n   * Scaling factor for the learning rate. A smaller learning rate may be useful to\r\n   * avoid overfitting.\r\n   */\r\n  learning_rate_multiplier?: 'auto' | number;\r\n\r\n  /**\r\n   * The number of epochs to train the model for. An epoch refers to one full cycle\r\n   * through the training dataset.\r\n   */\r\n  n_epochs?: 'auto' | number;\r\n}\r\n\r\n/**\r\n * Configuration for the DPO fine-tuning method.\r\n */\r\nexport interface DpoMethod {\r\n  /**\r\n   * The hyperparameters used for the DPO fine-tuning job.\r\n   */\r\n  hyperparameters?: DpoHyperparameters;\r\n}\r\n\r\n/**\r\n * The hyperparameters used for the reinforcement fine-tuning job.\r\n */\r\nexport interface ReinforcementHyperparameters {\r\n  /**\r\n   * Number of examples in each batch. A larger batch size means that model\r\n   * parameters are updated less frequently, but with lower variance.\r\n   */\r\n  batch_size?: 'auto' | number;\r\n\r\n  /**\r\n   * Multiplier on amount of compute used for exploring search space during training.\r\n   */\r\n  compute_multiplier?: 'auto' | number;\r\n\r\n  /**\r\n   * The number of training steps between evaluation runs.\r\n   */\r\n  eval_interval?: 'auto' | number;\r\n\r\n  /**\r\n   * Number of evaluation samples to generate per training step.\r\n   */\r\n  eval_samples?: 'auto' | number;\r\n\r\n  /**\r\n   * Scaling factor for the learning rate. A smaller learning rate may be useful to\r\n   * avoid overfitting.\r\n   */\r\n  learning_rate_multiplier?: 'auto' | number;\r\n\r\n  /**\r\n   * The number of epochs to train the model for. An epoch refers to one full cycle\r\n   * through the training dataset.\r\n   */\r\n  n_epochs?: 'auto' | number;\r\n\r\n  /**\r\n   * Level of reasoning effort.\r\n   */\r\n  reasoning_effort?: 'default' | 'low' | 'medium' | 'high';\r\n}\r\n\r\n/**\r\n * Configuration for the reinforcement fine-tuning method.\r\n */\r\nexport interface ReinforcementMethod {\r\n  /**\r\n   * The grader used for the fine-tuning job.\r\n   */\r\n  grader:\r\n    | GraderModelsAPI.StringCheckGrader\r\n    | GraderModelsAPI.TextSimilarityGrader\r\n    | GraderModelsAPI.PythonGrader\r\n    | GraderModelsAPI.ScoreModelGrader\r\n    | GraderModelsAPI.MultiGrader;\r\n\r\n  /**\r\n   * The hyperparameters used for the reinforcement fine-tuning job.\r\n   */\r\n  hyperparameters?: ReinforcementHyperparameters;\r\n}\r\n\r\n/**\r\n * The hyperparameters used for the fine-tuning job.\r\n */\r\nexport interface SupervisedHyperparameters {\r\n  /**\r\n   * Number of examples in each batch. A larger batch size means that model\r\n   * parameters are updated less frequently, but with lower variance.\r\n   */\r\n  batch_size?: 'auto' | number;\r\n\r\n  /**\r\n   * Scaling factor for the learning rate. A smaller learning rate may be useful to\r\n   * avoid overfitting.\r\n   */\r\n  learning_rate_multiplier?: 'auto' | number;\r\n\r\n  /**\r\n   * The number of epochs to train the model for. An epoch refers to one full cycle\r\n   * through the training dataset.\r\n   */\r\n  n_epochs?: 'auto' | number;\r\n}\r\n\r\n/**\r\n * Configuration for the supervised fine-tuning method.\r\n */\r\nexport interface SupervisedMethod {\r\n  /**\r\n   * The hyperparameters used for the fine-tuning job.\r\n   */\r\n  hyperparameters?: SupervisedHyperparameters;\r\n}\r\n\r\nexport declare namespace Methods {\r\n  export {\r\n    type DpoHyperparameters as DpoHyperparameters,\r\n    type DpoMethod as DpoMethod,\r\n    type ReinforcementHyperparameters as ReinforcementHyperparameters,\r\n    type ReinforcementMethod as ReinforcementMethod,\r\n    type SupervisedHyperparameters as SupervisedHyperparameters,\r\n    type SupervisedMethod as SupervisedMethod,\r\n  };\r\n}\r\n"],"mappings":"AAAA;SAESA,WAAW,QAAE;AAGtB,OAAM,MAAOC,OAAQ,SAAQD,WAAW","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}