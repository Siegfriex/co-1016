{"ast":null,"code":"import _objectSpread from \"C:/co-1016/node_modules/@babel/runtime/helpers/esm/objectSpread2.js\";\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../resource.mjs\";\nimport * as Core from \"../core.mjs\";\nexport class Images extends APIResource {\n  /**\r\n   * Creates a variation of a given image. This endpoint only supports `dall-e-2`.\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const imagesResponse = await client.images.createVariation({\r\n   *   image: fs.createReadStream('otter.png'),\r\n   * });\r\n   * ```\r\n   */\n  createVariation(body, options) {\n    return this._client.post('/images/variations', Core.multipartFormRequestOptions(_objectSpread({\n      body\n    }, options)));\n  }\n  /**\r\n   * Creates an edited or extended image given one or more source images and a\r\n   * prompt. This endpoint only supports `gpt-image-1` and `dall-e-2`.\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const imagesResponse = await client.images.edit({\r\n   *   image: fs.createReadStream('path/to/file'),\r\n   *   prompt: 'A cute baby sea otter wearing a beret',\r\n   * });\r\n   * ```\r\n   */\n  edit(body, options) {\n    return this._client.post('/images/edits', Core.multipartFormRequestOptions(_objectSpread({\n      body\n    }, options)));\n  }\n  /**\r\n   * Creates an image given a prompt.\r\n   * [Learn more](https://platform.openai.com/docs/guides/images).\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const imagesResponse = await client.images.generate({\r\n   *   prompt: 'A cute baby sea otter',\r\n   * });\r\n   * ```\r\n   */\n  generate(body, options) {\n    return this._client.post('/images/generations', _objectSpread({\n      body\n    }, options));\n  }\n}","map":{"version":3,"names":["APIResource","Core","Images","createVariation","body","options","_client","post","multipartFormRequestOptions","_objectSpread","edit","generate"],"sources":["C:\\co-1016\\node_modules\\openai\\src\\resources\\images.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\r\n\r\nimport { APIResource } from '../resource';\r\nimport * as Core from '../core';\r\n\r\nexport class Images extends APIResource {\r\n  /**\r\n   * Creates a variation of a given image. This endpoint only supports `dall-e-2`.\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const imagesResponse = await client.images.createVariation({\r\n   *   image: fs.createReadStream('otter.png'),\r\n   * });\r\n   * ```\r\n   */\r\n  createVariation(\r\n    body: ImageCreateVariationParams,\r\n    options?: Core.RequestOptions,\r\n  ): Core.APIPromise<ImagesResponse> {\r\n    return this._client.post('/images/variations', Core.multipartFormRequestOptions({ body, ...options }));\r\n  }\r\n\r\n  /**\r\n   * Creates an edited or extended image given one or more source images and a\r\n   * prompt. This endpoint only supports `gpt-image-1` and `dall-e-2`.\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const imagesResponse = await client.images.edit({\r\n   *   image: fs.createReadStream('path/to/file'),\r\n   *   prompt: 'A cute baby sea otter wearing a beret',\r\n   * });\r\n   * ```\r\n   */\r\n  edit(body: ImageEditParams, options?: Core.RequestOptions): Core.APIPromise<ImagesResponse> {\r\n    return this._client.post('/images/edits', Core.multipartFormRequestOptions({ body, ...options }));\r\n  }\r\n\r\n  /**\r\n   * Creates an image given a prompt.\r\n   * [Learn more](https://platform.openai.com/docs/guides/images).\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const imagesResponse = await client.images.generate({\r\n   *   prompt: 'A cute baby sea otter',\r\n   * });\r\n   * ```\r\n   */\r\n  generate(body: ImageGenerateParams, options?: Core.RequestOptions): Core.APIPromise<ImagesResponse> {\r\n    return this._client.post('/images/generations', { body, ...options });\r\n  }\r\n}\r\n\r\n/**\r\n * Represents the content or the URL of an image generated by the OpenAI API.\r\n */\r\nexport interface Image {\r\n  /**\r\n   * The base64-encoded JSON of the generated image. Default value for `gpt-image-1`,\r\n   * and only present if `response_format` is set to `b64_json` for `dall-e-2` and\r\n   * `dall-e-3`.\r\n   */\r\n  b64_json?: string;\r\n\r\n  /**\r\n   * For `dall-e-3` only, the revised prompt that was used to generate the image.\r\n   */\r\n  revised_prompt?: string;\r\n\r\n  /**\r\n   * When using `dall-e-2` or `dall-e-3`, the URL of the generated image if\r\n   * `response_format` is set to `url` (default value). Unsupported for\r\n   * `gpt-image-1`.\r\n   */\r\n  url?: string;\r\n}\r\n\r\nexport type ImageModel = 'dall-e-2' | 'dall-e-3' | 'gpt-image-1';\r\n\r\n/**\r\n * The response from the image generation endpoint.\r\n */\r\nexport interface ImagesResponse {\r\n  /**\r\n   * The Unix timestamp (in seconds) of when the image was created.\r\n   */\r\n  created: number;\r\n\r\n  /**\r\n   * The list of generated images.\r\n   */\r\n  data?: Array<Image>;\r\n\r\n  /**\r\n   * For `gpt-image-1` only, the token usage information for the image generation.\r\n   */\r\n  usage?: ImagesResponse.Usage;\r\n}\r\n\r\nexport namespace ImagesResponse {\r\n  /**\r\n   * For `gpt-image-1` only, the token usage information for the image generation.\r\n   */\r\n  export interface Usage {\r\n    /**\r\n     * The number of tokens (images and text) in the input prompt.\r\n     */\r\n    input_tokens: number;\r\n\r\n    /**\r\n     * The input tokens detailed information for the image generation.\r\n     */\r\n    input_tokens_details: Usage.InputTokensDetails;\r\n\r\n    /**\r\n     * The number of image tokens in the output image.\r\n     */\r\n    output_tokens: number;\r\n\r\n    /**\r\n     * The total number of tokens (images and text) used for the image generation.\r\n     */\r\n    total_tokens: number;\r\n  }\r\n\r\n  export namespace Usage {\r\n    /**\r\n     * The input tokens detailed information for the image generation.\r\n     */\r\n    export interface InputTokensDetails {\r\n      /**\r\n       * The number of image tokens in the input prompt.\r\n       */\r\n      image_tokens: number;\r\n\r\n      /**\r\n       * The number of text tokens in the input prompt.\r\n       */\r\n      text_tokens: number;\r\n    }\r\n  }\r\n}\r\n\r\nexport interface ImageCreateVariationParams {\r\n  /**\r\n   * The image to use as the basis for the variation(s). Must be a valid PNG file,\r\n   * less than 4MB, and square.\r\n   */\r\n  image: Core.Uploadable;\r\n\r\n  /**\r\n   * The model to use for image generation. Only `dall-e-2` is supported at this\r\n   * time.\r\n   */\r\n  model?: (string & {}) | ImageModel | null;\r\n\r\n  /**\r\n   * The number of images to generate. Must be between 1 and 10.\r\n   */\r\n  n?: number | null;\r\n\r\n  /**\r\n   * The format in which the generated images are returned. Must be one of `url` or\r\n   * `b64_json`. URLs are only valid for 60 minutes after the image has been\r\n   * generated.\r\n   */\r\n  response_format?: 'url' | 'b64_json' | null;\r\n\r\n  /**\r\n   * The size of the generated images. Must be one of `256x256`, `512x512`, or\r\n   * `1024x1024`.\r\n   */\r\n  size?: '256x256' | '512x512' | '1024x1024' | null;\r\n\r\n  /**\r\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\r\n   * and detect abuse.\r\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\r\n   */\r\n  user?: string;\r\n}\r\n\r\nexport interface ImageEditParams {\r\n  /**\r\n   * The image(s) to edit. Must be a supported image file or an array of images.\r\n   *\r\n   * For `gpt-image-1`, each image should be a `png`, `webp`, or `jpg` file less than\r\n   * 25MB. You can provide up to 16 images.\r\n   *\r\n   * For `dall-e-2`, you can only provide one image, and it should be a square `png`\r\n   * file less than 4MB.\r\n   */\r\n  image: Core.Uploadable | Array<Core.Uploadable>;\r\n\r\n  /**\r\n   * A text description of the desired image(s). The maximum length is 1000\r\n   * characters for `dall-e-2`, and 32000 characters for `gpt-image-1`.\r\n   */\r\n  prompt: string;\r\n\r\n  /**\r\n   * Allows to set transparency for the background of the generated image(s). This\r\n   * parameter is only supported for `gpt-image-1`. Must be one of `transparent`,\r\n   * `opaque` or `auto` (default value). When `auto` is used, the model will\r\n   * automatically determine the best background for the image.\r\n   *\r\n   * If `transparent`, the output format needs to support transparency, so it should\r\n   * be set to either `png` (default value) or `webp`.\r\n   */\r\n  background?: 'transparent' | 'opaque' | 'auto' | null;\r\n\r\n  /**\r\n   * An additional image whose fully transparent areas (e.g. where alpha is zero)\r\n   * indicate where `image` should be edited. If there are multiple images provided,\r\n   * the mask will be applied on the first image. Must be a valid PNG file, less than\r\n   * 4MB, and have the same dimensions as `image`.\r\n   */\r\n  mask?: Core.Uploadable;\r\n\r\n  /**\r\n   * The model to use for image generation. Only `dall-e-2` and `gpt-image-1` are\r\n   * supported. Defaults to `dall-e-2` unless a parameter specific to `gpt-image-1`\r\n   * is used.\r\n   */\r\n  model?: (string & {}) | ImageModel | null;\r\n\r\n  /**\r\n   * The number of images to generate. Must be between 1 and 10.\r\n   */\r\n  n?: number | null;\r\n\r\n  /**\r\n   * The quality of the image that will be generated. `high`, `medium` and `low` are\r\n   * only supported for `gpt-image-1`. `dall-e-2` only supports `standard` quality.\r\n   * Defaults to `auto`.\r\n   */\r\n  quality?: 'standard' | 'low' | 'medium' | 'high' | 'auto' | null;\r\n\r\n  /**\r\n   * The format in which the generated images are returned. Must be one of `url` or\r\n   * `b64_json`. URLs are only valid for 60 minutes after the image has been\r\n   * generated. This parameter is only supported for `dall-e-2`, as `gpt-image-1`\r\n   * will always return base64-encoded images.\r\n   */\r\n  response_format?: 'url' | 'b64_json' | null;\r\n\r\n  /**\r\n   * The size of the generated images. Must be one of `1024x1024`, `1536x1024`\r\n   * (landscape), `1024x1536` (portrait), or `auto` (default value) for\r\n   * `gpt-image-1`, and one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`.\r\n   */\r\n  size?: '256x256' | '512x512' | '1024x1024' | '1536x1024' | '1024x1536' | 'auto' | null;\r\n\r\n  /**\r\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\r\n   * and detect abuse.\r\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\r\n   */\r\n  user?: string;\r\n}\r\n\r\nexport interface ImageGenerateParams {\r\n  /**\r\n   * A text description of the desired image(s). The maximum length is 32000\r\n   * characters for `gpt-image-1`, 1000 characters for `dall-e-2` and 4000 characters\r\n   * for `dall-e-3`.\r\n   */\r\n  prompt: string;\r\n\r\n  /**\r\n   * Allows to set transparency for the background of the generated image(s). This\r\n   * parameter is only supported for `gpt-image-1`. Must be one of `transparent`,\r\n   * `opaque` or `auto` (default value). When `auto` is used, the model will\r\n   * automatically determine the best background for the image.\r\n   *\r\n   * If `transparent`, the output format needs to support transparency, so it should\r\n   * be set to either `png` (default value) or `webp`.\r\n   */\r\n  background?: 'transparent' | 'opaque' | 'auto' | null;\r\n\r\n  /**\r\n   * The model to use for image generation. One of `dall-e-2`, `dall-e-3`, or\r\n   * `gpt-image-1`. Defaults to `dall-e-2` unless a parameter specific to\r\n   * `gpt-image-1` is used.\r\n   */\r\n  model?: (string & {}) | ImageModel | null;\r\n\r\n  /**\r\n   * Control the content-moderation level for images generated by `gpt-image-1`. Must\r\n   * be either `low` for less restrictive filtering or `auto` (default value).\r\n   */\r\n  moderation?: 'low' | 'auto' | null;\r\n\r\n  /**\r\n   * The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only\r\n   * `n=1` is supported.\r\n   */\r\n  n?: number | null;\r\n\r\n  /**\r\n   * The compression level (0-100%) for the generated images. This parameter is only\r\n   * supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and\r\n   * defaults to 100.\r\n   */\r\n  output_compression?: number | null;\r\n\r\n  /**\r\n   * The format in which the generated images are returned. This parameter is only\r\n   * supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`.\r\n   */\r\n  output_format?: 'png' | 'jpeg' | 'webp' | null;\r\n\r\n  /**\r\n   * The quality of the image that will be generated.\r\n   *\r\n   * - `auto` (default value) will automatically select the best quality for the\r\n   *   given model.\r\n   * - `high`, `medium` and `low` are supported for `gpt-image-1`.\r\n   * - `hd` and `standard` are supported for `dall-e-3`.\r\n   * - `standard` is the only option for `dall-e-2`.\r\n   */\r\n  quality?: 'standard' | 'hd' | 'low' | 'medium' | 'high' | 'auto' | null;\r\n\r\n  /**\r\n   * The format in which generated images with `dall-e-2` and `dall-e-3` are\r\n   * returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes\r\n   * after the image has been generated. This parameter isn't supported for\r\n   * `gpt-image-1` which will always return base64-encoded images.\r\n   */\r\n  response_format?: 'url' | 'b64_json' | null;\r\n\r\n  /**\r\n   * The size of the generated images. Must be one of `1024x1024`, `1536x1024`\r\n   * (landscape), `1024x1536` (portrait), or `auto` (default value) for\r\n   * `gpt-image-1`, one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`, and\r\n   * one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3`.\r\n   */\r\n  size?:\r\n    | 'auto'\r\n    | '1024x1024'\r\n    | '1536x1024'\r\n    | '1024x1536'\r\n    | '256x256'\r\n    | '512x512'\r\n    | '1792x1024'\r\n    | '1024x1792'\r\n    | null;\r\n\r\n  /**\r\n   * The style of the generated images. This parameter is only supported for\r\n   * `dall-e-3`. Must be one of `vivid` or `natural`. Vivid causes the model to lean\r\n   * towards generating hyper-real and dramatic images. Natural causes the model to\r\n   * produce more natural, less hyper-real looking images.\r\n   */\r\n  style?: 'vivid' | 'natural' | null;\r\n\r\n  /**\r\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\r\n   * and detect abuse.\r\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\r\n   */\r\n  user?: string;\r\n}\r\n\r\nexport declare namespace Images {\r\n  export {\r\n    type Image as Image,\r\n    type ImageModel as ImageModel,\r\n    type ImagesResponse as ImagesResponse,\r\n    type ImageCreateVariationParams as ImageCreateVariationParams,\r\n    type ImageEditParams as ImageEditParams,\r\n    type ImageGenerateParams as ImageGenerateParams,\r\n  };\r\n}\r\n"],"mappings":";AAAA;SAESA,WAAW,QAAE;OACf,KAAKC,IAAI;AAEhB,OAAM,MAAOC,MAAO,SAAQF,WAAW;EACrC;;;;;;;;;;EAUAG,eAAeA,CACbC,IAAgC,EAChCC,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,oBAAoB,EAAEN,IAAI,CAACO,2BAA2B,CAAAC,aAAA;MAAGL;IAAI,GAAKC,OAAO,CAAE,CAAC,CAAC;EACxG;EAEA;;;;;;;;;;;;EAYAK,IAAIA,CAACN,IAAqB,EAAEC,OAA6B;IACvD,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,eAAe,EAAEN,IAAI,CAACO,2BAA2B,CAAAC,aAAA;MAAGL;IAAI,GAAKC,OAAO,CAAE,CAAC,CAAC;EACnG;EAEA;;;;;;;;;;;EAWAM,QAAQA,CAACP,IAAyB,EAAEC,OAA6B;IAC/D,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,qBAAqB,EAAAE,aAAA;MAAIL;IAAI,GAAKC,OAAO,CAAE,CAAC;EACvE","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}