{"ast":null,"code":"import _objectSpread from \"C:/co-1016/node_modules/@babel/runtime/helpers/esm/objectSpread2.js\";\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../resource.mjs\";\nexport class Completions extends APIResource {\n  create(body, options) {\n    var _body$stream;\n    return this._client.post('/completions', _objectSpread(_objectSpread({\n      body\n    }, options), {}, {\n      stream: (_body$stream = body.stream) !== null && _body$stream !== void 0 ? _body$stream : false\n    }));\n  }\n}","map":{"version":3,"names":["APIResource","Completions","create","body","options","_body$stream","_client","post","_objectSpread","stream"],"sources":["C:\\co-1016\\node_modules\\openai\\src\\resources\\completions.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\r\n\r\nimport { APIResource } from '../resource';\r\nimport { APIPromise } from '../core';\r\nimport * as Core from '../core';\r\nimport * as CompletionsAPI from './completions';\r\nimport * as CompletionsCompletionsAPI from './chat/completions/completions';\r\nimport { Stream } from '../streaming';\r\n\r\nexport class Completions extends APIResource {\r\n  /**\r\n   * Creates a completion for the provided prompt and parameters.\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const completion = await client.completions.create({\r\n   *   model: 'string',\r\n   *   prompt: 'This is a test.',\r\n   * });\r\n   * ```\r\n   */\r\n  create(body: CompletionCreateParamsNonStreaming, options?: Core.RequestOptions): APIPromise<Completion>;\r\n  create(\r\n    body: CompletionCreateParamsStreaming,\r\n    options?: Core.RequestOptions,\r\n  ): APIPromise<Stream<Completion>>;\r\n  create(\r\n    body: CompletionCreateParamsBase,\r\n    options?: Core.RequestOptions,\r\n  ): APIPromise<Stream<Completion> | Completion>;\r\n  create(\r\n    body: CompletionCreateParams,\r\n    options?: Core.RequestOptions,\r\n  ): APIPromise<Completion> | APIPromise<Stream<Completion>> {\r\n    return this._client.post('/completions', { body, ...options, stream: body.stream ?? false }) as\r\n      | APIPromise<Completion>\r\n      | APIPromise<Stream<Completion>>;\r\n  }\r\n}\r\n\r\n/**\r\n * Represents a completion response from the API. Note: both the streamed and\r\n * non-streamed response objects share the same shape (unlike the chat endpoint).\r\n */\r\nexport interface Completion {\r\n  /**\r\n   * A unique identifier for the completion.\r\n   */\r\n  id: string;\r\n\r\n  /**\r\n   * The list of completion choices the model generated for the input prompt.\r\n   */\r\n  choices: Array<CompletionChoice>;\r\n\r\n  /**\r\n   * The Unix timestamp (in seconds) of when the completion was created.\r\n   */\r\n  created: number;\r\n\r\n  /**\r\n   * The model used for completion.\r\n   */\r\n  model: string;\r\n\r\n  /**\r\n   * The object type, which is always \"text_completion\"\r\n   */\r\n  object: 'text_completion';\r\n\r\n  /**\r\n   * This fingerprint represents the backend configuration that the model runs with.\r\n   *\r\n   * Can be used in conjunction with the `seed` request parameter to understand when\r\n   * backend changes have been made that might impact determinism.\r\n   */\r\n  system_fingerprint?: string;\r\n\r\n  /**\r\n   * Usage statistics for the completion request.\r\n   */\r\n  usage?: CompletionUsage;\r\n}\r\n\r\nexport interface CompletionChoice {\r\n  /**\r\n   * The reason the model stopped generating tokens. This will be `stop` if the model\r\n   * hit a natural stop point or a provided stop sequence, `length` if the maximum\r\n   * number of tokens specified in the request was reached, or `content_filter` if\r\n   * content was omitted due to a flag from our content filters.\r\n   */\r\n  finish_reason: 'stop' | 'length' | 'content_filter';\r\n\r\n  index: number;\r\n\r\n  logprobs: CompletionChoice.Logprobs | null;\r\n\r\n  text: string;\r\n}\r\n\r\nexport namespace CompletionChoice {\r\n  export interface Logprobs {\r\n    text_offset?: Array<number>;\r\n\r\n    token_logprobs?: Array<number>;\r\n\r\n    tokens?: Array<string>;\r\n\r\n    top_logprobs?: Array<Record<string, number>>;\r\n  }\r\n}\r\n\r\n/**\r\n * Usage statistics for the completion request.\r\n */\r\nexport interface CompletionUsage {\r\n  /**\r\n   * Number of tokens in the generated completion.\r\n   */\r\n  completion_tokens: number;\r\n\r\n  /**\r\n   * Number of tokens in the prompt.\r\n   */\r\n  prompt_tokens: number;\r\n\r\n  /**\r\n   * Total number of tokens used in the request (prompt + completion).\r\n   */\r\n  total_tokens: number;\r\n\r\n  /**\r\n   * Breakdown of tokens used in a completion.\r\n   */\r\n  completion_tokens_details?: CompletionUsage.CompletionTokensDetails;\r\n\r\n  /**\r\n   * Breakdown of tokens used in the prompt.\r\n   */\r\n  prompt_tokens_details?: CompletionUsage.PromptTokensDetails;\r\n}\r\n\r\nexport namespace CompletionUsage {\r\n  /**\r\n   * Breakdown of tokens used in a completion.\r\n   */\r\n  export interface CompletionTokensDetails {\r\n    /**\r\n     * When using Predicted Outputs, the number of tokens in the prediction that\r\n     * appeared in the completion.\r\n     */\r\n    accepted_prediction_tokens?: number;\r\n\r\n    /**\r\n     * Audio input tokens generated by the model.\r\n     */\r\n    audio_tokens?: number;\r\n\r\n    /**\r\n     * Tokens generated by the model for reasoning.\r\n     */\r\n    reasoning_tokens?: number;\r\n\r\n    /**\r\n     * When using Predicted Outputs, the number of tokens in the prediction that did\r\n     * not appear in the completion. However, like reasoning tokens, these tokens are\r\n     * still counted in the total completion tokens for purposes of billing, output,\r\n     * and context window limits.\r\n     */\r\n    rejected_prediction_tokens?: number;\r\n  }\r\n\r\n  /**\r\n   * Breakdown of tokens used in the prompt.\r\n   */\r\n  export interface PromptTokensDetails {\r\n    /**\r\n     * Audio input tokens present in the prompt.\r\n     */\r\n    audio_tokens?: number;\r\n\r\n    /**\r\n     * Cached tokens present in the prompt.\r\n     */\r\n    cached_tokens?: number;\r\n  }\r\n}\r\n\r\nexport type CompletionCreateParams = CompletionCreateParamsNonStreaming | CompletionCreateParamsStreaming;\r\n\r\nexport interface CompletionCreateParamsBase {\r\n  /**\r\n   * ID of the model to use. You can use the\r\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\r\n   * see all of your available models, or see our\r\n   * [Model overview](https://platform.openai.com/docs/models) for descriptions of\r\n   * them.\r\n   */\r\n  model: (string & {}) | 'gpt-3.5-turbo-instruct' | 'davinci-002' | 'babbage-002';\r\n\r\n  /**\r\n   * The prompt(s) to generate completions for, encoded as a string, array of\r\n   * strings, array of tokens, or array of token arrays.\r\n   *\r\n   * Note that <|endoftext|> is the document separator that the model sees during\r\n   * training, so if a prompt is not specified the model will generate as if from the\r\n   * beginning of a new document.\r\n   */\r\n  prompt: string | Array<string> | Array<number> | Array<Array<number>> | null;\r\n\r\n  /**\r\n   * Generates `best_of` completions server-side and returns the \"best\" (the one with\r\n   * the highest log probability per token). Results cannot be streamed.\r\n   *\r\n   * When used with `n`, `best_of` controls the number of candidate completions and\r\n   * `n` specifies how many to return â€“ `best_of` must be greater than `n`.\r\n   *\r\n   * **Note:** Because this parameter generates many completions, it can quickly\r\n   * consume your token quota. Use carefully and ensure that you have reasonable\r\n   * settings for `max_tokens` and `stop`.\r\n   */\r\n  best_of?: number | null;\r\n\r\n  /**\r\n   * Echo back the prompt in addition to the completion\r\n   */\r\n  echo?: boolean | null;\r\n\r\n  /**\r\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\r\n   * existing frequency in the text so far, decreasing the model's likelihood to\r\n   * repeat the same line verbatim.\r\n   *\r\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)\r\n   */\r\n  frequency_penalty?: number | null;\r\n\r\n  /**\r\n   * Modify the likelihood of specified tokens appearing in the completion.\r\n   *\r\n   * Accepts a JSON object that maps tokens (specified by their token ID in the GPT\r\n   * tokenizer) to an associated bias value from -100 to 100. You can use this\r\n   * [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.\r\n   * Mathematically, the bias is added to the logits generated by the model prior to\r\n   * sampling. The exact effect will vary per model, but values between -1 and 1\r\n   * should decrease or increase likelihood of selection; values like -100 or 100\r\n   * should result in a ban or exclusive selection of the relevant token.\r\n   *\r\n   * As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token\r\n   * from being generated.\r\n   */\r\n  logit_bias?: Record<string, number> | null;\r\n\r\n  /**\r\n   * Include the log probabilities on the `logprobs` most likely output tokens, as\r\n   * well the chosen tokens. For example, if `logprobs` is 5, the API will return a\r\n   * list of the 5 most likely tokens. The API will always return the `logprob` of\r\n   * the sampled token, so there may be up to `logprobs+1` elements in the response.\r\n   *\r\n   * The maximum value for `logprobs` is 5.\r\n   */\r\n  logprobs?: number | null;\r\n\r\n  /**\r\n   * The maximum number of [tokens](/tokenizer) that can be generated in the\r\n   * completion.\r\n   *\r\n   * The token count of your prompt plus `max_tokens` cannot exceed the model's\r\n   * context length.\r\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\r\n   * for counting tokens.\r\n   */\r\n  max_tokens?: number | null;\r\n\r\n  /**\r\n   * How many completions to generate for each prompt.\r\n   *\r\n   * **Note:** Because this parameter generates many completions, it can quickly\r\n   * consume your token quota. Use carefully and ensure that you have reasonable\r\n   * settings for `max_tokens` and `stop`.\r\n   */\r\n  n?: number | null;\r\n\r\n  /**\r\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\r\n   * whether they appear in the text so far, increasing the model's likelihood to\r\n   * talk about new topics.\r\n   *\r\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)\r\n   */\r\n  presence_penalty?: number | null;\r\n\r\n  /**\r\n   * If specified, our system will make a best effort to sample deterministically,\r\n   * such that repeated requests with the same `seed` and parameters should return\r\n   * the same result.\r\n   *\r\n   * Determinism is not guaranteed, and you should refer to the `system_fingerprint`\r\n   * response parameter to monitor changes in the backend.\r\n   */\r\n  seed?: number | null;\r\n\r\n  /**\r\n   * Not supported with latest reasoning models `o3` and `o4-mini`.\r\n   *\r\n   * Up to 4 sequences where the API will stop generating further tokens. The\r\n   * returned text will not contain the stop sequence.\r\n   */\r\n  stop?: string | null | Array<string>;\r\n\r\n  /**\r\n   * Whether to stream back partial progress. If set, tokens will be sent as\r\n   * data-only\r\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\r\n   * as they become available, with the stream terminated by a `data: [DONE]`\r\n   * message.\r\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\r\n   */\r\n  stream?: boolean | null;\r\n\r\n  /**\r\n   * Options for streaming response. Only set this when you set `stream: true`.\r\n   */\r\n  stream_options?: CompletionsCompletionsAPI.ChatCompletionStreamOptions | null;\r\n\r\n  /**\r\n   * The suffix that comes after a completion of inserted text.\r\n   *\r\n   * This parameter is only supported for `gpt-3.5-turbo-instruct`.\r\n   */\r\n  suffix?: string | null;\r\n\r\n  /**\r\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\r\n   * make the output more random, while lower values like 0.2 will make it more\r\n   * focused and deterministic.\r\n   *\r\n   * We generally recommend altering this or `top_p` but not both.\r\n   */\r\n  temperature?: number | null;\r\n\r\n  /**\r\n   * An alternative to sampling with temperature, called nucleus sampling, where the\r\n   * model considers the results of the tokens with top_p probability mass. So 0.1\r\n   * means only the tokens comprising the top 10% probability mass are considered.\r\n   *\r\n   * We generally recommend altering this or `temperature` but not both.\r\n   */\r\n  top_p?: number | null;\r\n\r\n  /**\r\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\r\n   * and detect abuse.\r\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\r\n   */\r\n  user?: string;\r\n}\r\n\r\nexport namespace CompletionCreateParams {\r\n  export type CompletionCreateParamsNonStreaming = CompletionsAPI.CompletionCreateParamsNonStreaming;\r\n  export type CompletionCreateParamsStreaming = CompletionsAPI.CompletionCreateParamsStreaming;\r\n}\r\n\r\nexport interface CompletionCreateParamsNonStreaming extends CompletionCreateParamsBase {\r\n  /**\r\n   * Whether to stream back partial progress. If set, tokens will be sent as\r\n   * data-only\r\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\r\n   * as they become available, with the stream terminated by a `data: [DONE]`\r\n   * message.\r\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\r\n   */\r\n  stream?: false | null;\r\n}\r\n\r\nexport interface CompletionCreateParamsStreaming extends CompletionCreateParamsBase {\r\n  /**\r\n   * Whether to stream back partial progress. If set, tokens will be sent as\r\n   * data-only\r\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\r\n   * as they become available, with the stream terminated by a `data: [DONE]`\r\n   * message.\r\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\r\n   */\r\n  stream: true;\r\n}\r\n\r\nexport declare namespace Completions {\r\n  export {\r\n    type Completion as Completion,\r\n    type CompletionChoice as CompletionChoice,\r\n    type CompletionUsage as CompletionUsage,\r\n    type CompletionCreateParams as CompletionCreateParams,\r\n    type CompletionCreateParamsNonStreaming as CompletionCreateParamsNonStreaming,\r\n    type CompletionCreateParamsStreaming as CompletionCreateParamsStreaming,\r\n  };\r\n}\r\n"],"mappings":";AAAA;SAESA,WAAW,QAAE;AAOtB,OAAM,MAAOC,WAAY,SAAQD,WAAW;EAqB1CE,MAAMA,CACJC,IAA4B,EAC5BC,OAA6B;IAAA,IAAAC,YAAA;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,cAAc,EAAAC,aAAA,CAAAA,aAAA;MAAIL;IAAI,GAAKC,OAAO;MAAEK,MAAM,GAAAJ,YAAA,GAAEF,IAAI,CAACM,MAAM,cAAAJ,YAAA,cAAAA,YAAA,GAAI;IAAK,EAAE,CAEzD;EACpC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}