{"ast":null,"code":"import _objectSpread from \"C:/co-1016/node_modules/@babel/runtime/helpers/esm/objectSpread2.js\";\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../../resource.mjs\";\nimport { isRequestOptions } from \"../../core.mjs\";\nimport { sleep } from \"../../core.mjs\";\nimport { allSettledWithThrow } from \"../../lib/Util.mjs\";\nimport { VectorStoreFilesPage } from \"./files.mjs\";\nexport class FileBatches extends APIResource {\n  /**\r\n   * Create a vector store file batch.\r\n   */\n  create(vectorStoreId, body, options) {\n    return this._client.post(\"/vector_stores/\".concat(vectorStoreId, \"/file_batches\"), _objectSpread(_objectSpread({\n      body\n    }, options), {}, {\n      headers: _objectSpread({\n        'OpenAI-Beta': 'assistants=v2'\n      }, options === null || options === void 0 ? void 0 : options.headers)\n    }));\n  }\n  /**\r\n   * Retrieves a vector store file batch.\r\n   */\n  retrieve(vectorStoreId, batchId, options) {\n    return this._client.get(\"/vector_stores/\".concat(vectorStoreId, \"/file_batches/\").concat(batchId), _objectSpread(_objectSpread({}, options), {}, {\n      headers: _objectSpread({\n        'OpenAI-Beta': 'assistants=v2'\n      }, options === null || options === void 0 ? void 0 : options.headers)\n    }));\n  }\n  /**\r\n   * Cancel a vector store file batch. This attempts to cancel the processing of\r\n   * files in this batch as soon as possible.\r\n   */\n  cancel(vectorStoreId, batchId, options) {\n    return this._client.post(\"/vector_stores/\".concat(vectorStoreId, \"/file_batches/\").concat(batchId, \"/cancel\"), _objectSpread(_objectSpread({}, options), {}, {\n      headers: _objectSpread({\n        'OpenAI-Beta': 'assistants=v2'\n      }, options === null || options === void 0 ? void 0 : options.headers)\n    }));\n  }\n  /**\r\n   * Create a vector store batch and poll until all files have been processed.\r\n   */\n  async createAndPoll(vectorStoreId, body, options) {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n  listFiles(vectorStoreId, batchId) {\n    let query = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    let options = arguments.length > 3 ? arguments[3] : undefined;\n    if (isRequestOptions(query)) {\n      return this.listFiles(vectorStoreId, batchId, {}, query);\n    }\n    return this._client.getAPIList(\"/vector_stores/\".concat(vectorStoreId, \"/file_batches/\").concat(batchId, \"/files\"), VectorStoreFilesPage, _objectSpread(_objectSpread({\n      query\n    }, options), {}, {\n      headers: _objectSpread({\n        'OpenAI-Beta': 'assistants=v2'\n      }, options === null || options === void 0 ? void 0 : options.headers)\n    }));\n  }\n  /**\r\n   * Wait for the given file batch to be processed.\r\n   *\r\n   * Note: this will return even if one of the files failed to process, you need to\r\n   * check batch.file_counts.failed_count to handle this case.\r\n   */\n  async poll(vectorStoreId, batchId, options) {\n    const headers = _objectSpread(_objectSpread({}, options === null || options === void 0 ? void 0 : options.headers), {}, {\n      'X-Stainless-Poll-Helper': 'true'\n    });\n    if (options !== null && options !== void 0 && options.pollIntervalMs) {\n      headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n    }\n    while (true) {\n      const {\n        data: batch,\n        response\n      } = await this.retrieve(vectorStoreId, batchId, _objectSpread(_objectSpread({}, options), {}, {\n        headers\n      })).withResponse();\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n          if (options !== null && options !== void 0 && options.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n  /**\r\n   * Uploads the given files concurrently and then creates a vector store file batch.\r\n   *\r\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\r\n   */\n  async uploadAndPoll(vectorStoreId, _ref, options) {\n    var _options$maxConcurren;\n    let {\n      files,\n      fileIds = []\n    } = _ref;\n    if (files == null || files.length == 0) {\n      throw new Error(\"No `files` provided to process. If you've already uploaded files you should use `.createAndPoll()` instead\");\n    }\n    const configuredConcurrency = (_options$maxConcurren = options === null || options === void 0 ? void 0 : options.maxConcurrency) !== null && _options$maxConcurren !== void 0 ? _options$maxConcurren : 5;\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds = [...fileIds];\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({\n          file: item,\n          purpose: 'assistants'\n        }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n    // Wait for all processing to complete.\n    await allSettledWithThrow(workers);\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds\n    });\n  }\n}\nexport { VectorStoreFilesPage };","map":{"version":3,"names":["APIResource","isRequestOptions","sleep","allSettledWithThrow","VectorStoreFilesPage","FileBatches","create","vectorStoreId","body","options","_client","post","concat","_objectSpread","headers","retrieve","batchId","get","cancel","createAndPoll","batch","poll","id","listFiles","query","arguments","length","undefined","getAPIList","pollIntervalMs","toString","data","response","withResponse","status","sleepInterval","headerInterval","headerIntervalMs","parseInt","isNaN","uploadAndPoll","_ref","_options$maxConcurren","files","fileIds","Error","configuredConcurrency","maxConcurrency","concurrencyLimit","Math","min","client","fileIterator","values","allFileIds","processFiles","iterator","item","fileObj","file","purpose","push","workers","Array","fill","map","file_ids"],"sources":["C:\\co-1016\\node_modules\\openai\\src\\resources\\vector-stores\\file-batches.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\r\n\r\nimport { APIResource } from '../../resource';\r\nimport { isRequestOptions } from '../../core';\r\nimport { sleep } from '../../core';\r\nimport { Uploadable } from '../../core';\r\nimport { allSettledWithThrow } from '../../lib/Util';\r\nimport * as Core from '../../core';\r\nimport * as FilesAPI from './files';\r\nimport { VectorStoreFilesPage } from './files';\r\nimport * as VectorStoresAPI from './vector-stores';\r\nimport { type CursorPageParams } from '../../pagination';\r\n\r\nexport class FileBatches extends APIResource {\r\n  /**\r\n   * Create a vector store file batch.\r\n   */\r\n  create(\r\n    vectorStoreId: string,\r\n    body: FileBatchCreateParams,\r\n    options?: Core.RequestOptions,\r\n  ): Core.APIPromise<VectorStoreFileBatch> {\r\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches`, {\r\n      body,\r\n      ...options,\r\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Retrieves a vector store file batch.\r\n   */\r\n  retrieve(\r\n    vectorStoreId: string,\r\n    batchId: string,\r\n    options?: Core.RequestOptions,\r\n  ): Core.APIPromise<VectorStoreFileBatch> {\r\n    return this._client.get(`/vector_stores/${vectorStoreId}/file_batches/${batchId}`, {\r\n      ...options,\r\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Cancel a vector store file batch. This attempts to cancel the processing of\r\n   * files in this batch as soon as possible.\r\n   */\r\n  cancel(\r\n    vectorStoreId: string,\r\n    batchId: string,\r\n    options?: Core.RequestOptions,\r\n  ): Core.APIPromise<VectorStoreFileBatch> {\r\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/cancel`, {\r\n      ...options,\r\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Create a vector store batch and poll until all files have been processed.\r\n   */\r\n  async createAndPoll(\r\n    vectorStoreId: string,\r\n    body: FileBatchCreateParams,\r\n    options?: Core.RequestOptions & { pollIntervalMs?: number },\r\n  ): Promise<VectorStoreFileBatch> {\r\n    const batch = await this.create(vectorStoreId, body);\r\n    return await this.poll(vectorStoreId, batch.id, options);\r\n  }\r\n\r\n  /**\r\n   * Returns a list of vector store files in a batch.\r\n   */\r\n  listFiles(\r\n    vectorStoreId: string,\r\n    batchId: string,\r\n    query?: FileBatchListFilesParams,\r\n    options?: Core.RequestOptions,\r\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile>;\r\n  listFiles(\r\n    vectorStoreId: string,\r\n    batchId: string,\r\n    options?: Core.RequestOptions,\r\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile>;\r\n  listFiles(\r\n    vectorStoreId: string,\r\n    batchId: string,\r\n    query: FileBatchListFilesParams | Core.RequestOptions = {},\r\n    options?: Core.RequestOptions,\r\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile> {\r\n    if (isRequestOptions(query)) {\r\n      return this.listFiles(vectorStoreId, batchId, {}, query);\r\n    }\r\n    return this._client.getAPIList(\r\n      `/vector_stores/${vectorStoreId}/file_batches/${batchId}/files`,\r\n      VectorStoreFilesPage,\r\n      { query, ...options, headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers } },\r\n    );\r\n  }\r\n\r\n  /**\r\n   * Wait for the given file batch to be processed.\r\n   *\r\n   * Note: this will return even if one of the files failed to process, you need to\r\n   * check batch.file_counts.failed_count to handle this case.\r\n   */\r\n  async poll(\r\n    vectorStoreId: string,\r\n    batchId: string,\r\n    options?: Core.RequestOptions & { pollIntervalMs?: number },\r\n  ): Promise<VectorStoreFileBatch> {\r\n    const headers: { [key: string]: string } = { ...options?.headers, 'X-Stainless-Poll-Helper': 'true' };\r\n    if (options?.pollIntervalMs) {\r\n      headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\r\n    }\r\n\r\n    while (true) {\r\n      const { data: batch, response } = await this.retrieve(vectorStoreId, batchId, {\r\n        ...options,\r\n        headers,\r\n      }).withResponse();\r\n\r\n      switch (batch.status) {\r\n        case 'in_progress':\r\n          let sleepInterval = 5000;\r\n\r\n          if (options?.pollIntervalMs) {\r\n            sleepInterval = options.pollIntervalMs;\r\n          } else {\r\n            const headerInterval = response.headers.get('openai-poll-after-ms');\r\n            if (headerInterval) {\r\n              const headerIntervalMs = parseInt(headerInterval);\r\n              if (!isNaN(headerIntervalMs)) {\r\n                sleepInterval = headerIntervalMs;\r\n              }\r\n            }\r\n          }\r\n          await sleep(sleepInterval);\r\n          break;\r\n        case 'failed':\r\n        case 'cancelled':\r\n        case 'completed':\r\n          return batch;\r\n      }\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Uploads the given files concurrently and then creates a vector store file batch.\r\n   *\r\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\r\n   */\r\n  async uploadAndPoll(\r\n    vectorStoreId: string,\r\n    { files, fileIds = [] }: { files: Uploadable[]; fileIds?: string[] },\r\n    options?: Core.RequestOptions & { pollIntervalMs?: number; maxConcurrency?: number },\r\n  ): Promise<VectorStoreFileBatch> {\r\n    if (files == null || files.length == 0) {\r\n      throw new Error(\r\n        `No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`,\r\n      );\r\n    }\r\n\r\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\r\n\r\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\r\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\r\n\r\n    const client = this._client;\r\n    const fileIterator = files.values();\r\n    const allFileIds: string[] = [...fileIds];\r\n\r\n    // This code is based on this design. The libraries don't accommodate our environment limits.\r\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\r\n    async function processFiles(iterator: IterableIterator<Uploadable>) {\r\n      for (let item of iterator) {\r\n        const fileObj = await client.files.create({ file: item, purpose: 'assistants' }, options);\r\n        allFileIds.push(fileObj.id);\r\n      }\r\n    }\r\n\r\n    // Start workers to process results\r\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\r\n\r\n    // Wait for all processing to complete.\r\n    await allSettledWithThrow(workers);\r\n\r\n    return await this.createAndPoll(vectorStoreId, {\r\n      file_ids: allFileIds,\r\n    });\r\n  }\r\n}\r\n\r\n/**\r\n * A batch of files attached to a vector store.\r\n */\r\nexport interface VectorStoreFileBatch {\r\n  /**\r\n   * The identifier, which can be referenced in API endpoints.\r\n   */\r\n  id: string;\r\n\r\n  /**\r\n   * The Unix timestamp (in seconds) for when the vector store files batch was\r\n   * created.\r\n   */\r\n  created_at: number;\r\n\r\n  file_counts: VectorStoreFileBatch.FileCounts;\r\n\r\n  /**\r\n   * The object type, which is always `vector_store.file_batch`.\r\n   */\r\n  object: 'vector_store.files_batch';\r\n\r\n  /**\r\n   * The status of the vector store files batch, which can be either `in_progress`,\r\n   * `completed`, `cancelled` or `failed`.\r\n   */\r\n  status: 'in_progress' | 'completed' | 'cancelled' | 'failed';\r\n\r\n  /**\r\n   * The ID of the\r\n   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\r\n   * that the [File](https://platform.openai.com/docs/api-reference/files) is\r\n   * attached to.\r\n   */\r\n  vector_store_id: string;\r\n}\r\n\r\nexport namespace VectorStoreFileBatch {\r\n  export interface FileCounts {\r\n    /**\r\n     * The number of files that where cancelled.\r\n     */\r\n    cancelled: number;\r\n\r\n    /**\r\n     * The number of files that have been processed.\r\n     */\r\n    completed: number;\r\n\r\n    /**\r\n     * The number of files that have failed to process.\r\n     */\r\n    failed: number;\r\n\r\n    /**\r\n     * The number of files that are currently being processed.\r\n     */\r\n    in_progress: number;\r\n\r\n    /**\r\n     * The total number of files.\r\n     */\r\n    total: number;\r\n  }\r\n}\r\n\r\nexport interface FileBatchCreateParams {\r\n  /**\r\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\r\n   * the vector store should use. Useful for tools like `file_search` that can access\r\n   * files.\r\n   */\r\n  file_ids: Array<string>;\r\n\r\n  /**\r\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\r\n   * for storing additional information about the object in a structured format, and\r\n   * querying for objects via API or the dashboard. Keys are strings with a maximum\r\n   * length of 64 characters. Values are strings with a maximum length of 512\r\n   * characters, booleans, or numbers.\r\n   */\r\n  attributes?: Record<string, string | number | boolean> | null;\r\n\r\n  /**\r\n   * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\r\n   * strategy. Only applicable if `file_ids` is non-empty.\r\n   */\r\n  chunking_strategy?: VectorStoresAPI.FileChunkingStrategyParam;\r\n}\r\n\r\nexport interface FileBatchListFilesParams extends CursorPageParams {\r\n  /**\r\n   * A cursor for use in pagination. `before` is an object ID that defines your place\r\n   * in the list. For instance, if you make a list request and receive 100 objects,\r\n   * starting with obj_foo, your subsequent call can include before=obj_foo in order\r\n   * to fetch the previous page of the list.\r\n   */\r\n  before?: string;\r\n\r\n  /**\r\n   * Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.\r\n   */\r\n  filter?: 'in_progress' | 'completed' | 'failed' | 'cancelled';\r\n\r\n  /**\r\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\r\n   * order and `desc` for descending order.\r\n   */\r\n  order?: 'asc' | 'desc';\r\n}\r\n\r\nexport declare namespace FileBatches {\r\n  export {\r\n    type VectorStoreFileBatch as VectorStoreFileBatch,\r\n    type FileBatchCreateParams as FileBatchCreateParams,\r\n    type FileBatchListFilesParams as FileBatchListFilesParams,\r\n  };\r\n}\r\n\r\nexport { VectorStoreFilesPage };\r\n"],"mappings":";AAAA;SAESA,WAAW,QAAE;SACbC,gBAAgB,QAAE;SAClBC,KAAK,QAAE;SAEPC,mBAAmB,QAAE;SAGrBC,oBAAoB,QAAE;AAI/B,OAAM,MAAOC,WAAY,SAAQL,WAAW;EAC1C;;;EAGAM,MAAMA,CACJC,aAAqB,EACrBC,IAA2B,EAC3BC,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,mBAAAC,MAAA,CAAmBL,aAAa,oBAAAM,aAAA,CAAAA,aAAA;MACtDL;IAAI,GACDC,OAAO;MACVK,OAAO,EAAAD,aAAA;QAAI,aAAa,EAAE;MAAe,GAAKJ,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEK,OAAO;IAAE,EACjE,CAAC;EACJ;EAEA;;;EAGAC,QAAQA,CACNR,aAAqB,EACrBS,OAAe,EACfP,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACO,GAAG,mBAAAL,MAAA,CAAmBL,aAAa,oBAAAK,MAAA,CAAiBI,OAAO,GAAAH,aAAA,CAAAA,aAAA,KAC1EJ,OAAO;MACVK,OAAO,EAAAD,aAAA;QAAI,aAAa,EAAE;MAAe,GAAKJ,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEK,OAAO;IAAE,EACjE,CAAC;EACJ;EAEA;;;;EAIAI,MAAMA,CACJX,aAAqB,EACrBS,OAAe,EACfP,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,mBAAAC,MAAA,CAAmBL,aAAa,oBAAAK,MAAA,CAAiBI,OAAO,cAAAH,aAAA,CAAAA,aAAA,KAC3EJ,OAAO;MACVK,OAAO,EAAAD,aAAA;QAAI,aAAa,EAAE;MAAe,GAAKJ,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEK,OAAO;IAAE,EACjE,CAAC;EACJ;EAEA;;;EAGA,MAAMK,aAAaA,CACjBZ,aAAqB,EACrBC,IAA2B,EAC3BC,OAA2D;IAE3D,MAAMW,KAAK,GAAG,MAAM,IAAI,CAACd,MAAM,CAACC,aAAa,EAAEC,IAAI,CAAC;IACpD,OAAO,MAAM,IAAI,CAACa,IAAI,CAACd,aAAa,EAAEa,KAAK,CAACE,EAAE,EAAEb,OAAO,CAAC;EAC1D;EAgBAc,SAASA,CACPhB,aAAqB,EACrBS,OAAe,EAEc;IAAA,IAD7BQ,KAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAwD,EAAE;IAAA,IAC1DhB,OAA6B,GAAAgB,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;IAE7B,IAAI1B,gBAAgB,CAACuB,KAAK,CAAC,EAAE;MAC3B,OAAO,IAAI,CAACD,SAAS,CAAChB,aAAa,EAAES,OAAO,EAAE,EAAE,EAAEQ,KAAK,CAAC;;IAE1D,OAAO,IAAI,CAACd,OAAO,CAACkB,UAAU,mBAAAhB,MAAA,CACVL,aAAa,oBAAAK,MAAA,CAAiBI,OAAO,aACvDZ,oBAAoB,EAAAS,aAAA,CAAAA,aAAA;MAClBW;IAAK,GAAKf,OAAO;MAAEK,OAAO,EAAAD,aAAA;QAAI,aAAa,EAAE;MAAe,GAAKJ,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEK,OAAO;IAAE,EAAE,CACxF;EACH;EAEA;;;;;;EAMA,MAAMO,IAAIA,CACRd,aAAqB,EACrBS,OAAe,EACfP,OAA2D;IAE3D,MAAMK,OAAO,GAAAD,aAAA,CAAAA,aAAA,KAAmCJ,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEK,OAAO;MAAE,yBAAyB,EAAE;IAAM,EAAE;IACrG,IAAIL,OAAO,aAAPA,OAAO,eAAPA,OAAO,CAAEoB,cAAc,EAAE;MAC3Bf,OAAO,CAAC,kCAAkC,CAAC,GAAGL,OAAO,CAACoB,cAAc,CAACC,QAAQ,EAAE;;IAGjF,OAAO,IAAI,EAAE;MACX,MAAM;QAAEC,IAAI,EAAEX,KAAK;QAAEY;MAAQ,CAAE,GAAG,MAAM,IAAI,CAACjB,QAAQ,CAACR,aAAa,EAAES,OAAO,EAAAH,aAAA,CAAAA,aAAA,KACvEJ,OAAO;QACVK;MAAO,EACR,CAAC,CAACmB,YAAY,EAAE;MAEjB,QAAQb,KAAK,CAACc,MAAM;QAClB,KAAK,aAAa;UAChB,IAAIC,aAAa,GAAG,IAAI;UAExB,IAAI1B,OAAO,aAAPA,OAAO,eAAPA,OAAO,CAAEoB,cAAc,EAAE;YAC3BM,aAAa,GAAG1B,OAAO,CAACoB,cAAc;WACvC,MAAM;YACL,MAAMO,cAAc,GAAGJ,QAAQ,CAAClB,OAAO,CAACG,GAAG,CAAC,sBAAsB,CAAC;YACnE,IAAImB,cAAc,EAAE;cAClB,MAAMC,gBAAgB,GAAGC,QAAQ,CAACF,cAAc,CAAC;cACjD,IAAI,CAACG,KAAK,CAACF,gBAAgB,CAAC,EAAE;gBAC5BF,aAAa,GAAGE,gBAAgB;;;;UAItC,MAAMnC,KAAK,CAACiC,aAAa,CAAC;UAC1B;QACF,KAAK,QAAQ;QACb,KAAK,WAAW;QAChB,KAAK,WAAW;UACd,OAAOf,KAAK;;;EAGpB;EAEA;;;;;EAKA,MAAMoB,aAAaA,CACjBjC,aAAqB,EAAAkC,IAAA,EAErBhC,OAAoF;IAAA,IAAAiC,qBAAA;IAAA,IADpF;MAAEC,KAAK;MAAEC,OAAO,GAAG;IAAE,CAA+C,GAAAH,IAAA;IAGpE,IAAIE,KAAK,IAAI,IAAI,IAAIA,KAAK,CAACjB,MAAM,IAAI,CAAC,EAAE;MACtC,MAAM,IAAImB,KAAK,6GACmG,CACjH;;IAGH,MAAMC,qBAAqB,IAAAJ,qBAAA,GAAGjC,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEsC,cAAc,cAAAL,qBAAA,cAAAA,qBAAA,GAAI,CAAC;IAE1D;IACA,MAAMM,gBAAgB,GAAGC,IAAI,CAACC,GAAG,CAACJ,qBAAqB,EAAEH,KAAK,CAACjB,MAAM,CAAC;IAEtE,MAAMyB,MAAM,GAAG,IAAI,CAACzC,OAAO;IAC3B,MAAM0C,YAAY,GAAGT,KAAK,CAACU,MAAM,EAAE;IACnC,MAAMC,UAAU,GAAa,CAAC,GAAGV,OAAO,CAAC;IAEzC;IACA;IACA,eAAeW,YAAYA,CAACC,QAAsC;MAChE,KAAK,IAAIC,IAAI,IAAID,QAAQ,EAAE;QACzB,MAAME,OAAO,GAAG,MAAMP,MAAM,CAACR,KAAK,CAACrC,MAAM,CAAC;UAAEqD,IAAI,EAAEF,IAAI;UAAEG,OAAO,EAAE;QAAY,CAAE,EAAEnD,OAAO,CAAC;QACzF6C,UAAU,CAACO,IAAI,CAACH,OAAO,CAACpC,EAAE,CAAC;;IAE/B;IAEA;IACA,MAAMwC,OAAO,GAAGC,KAAK,CAACf,gBAAgB,CAAC,CAACgB,IAAI,CAACZ,YAAY,CAAC,CAACa,GAAG,CAACV,YAAY,CAAC;IAE5E;IACA,MAAMpD,mBAAmB,CAAC2D,OAAO,CAAC;IAElC,OAAO,MAAM,IAAI,CAAC3C,aAAa,CAACZ,aAAa,EAAE;MAC7C2D,QAAQ,EAAEZ;KACX,CAAC;EACJ;;AA0HF,SAASlD,oBAAoB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}